{
  "hash": "87beb737765e457d86272760ed64af70",
  "result": {
    "markdown": "---\ntitle: Clustering\nauthor: Rithvik Guntor\ndate: 2023-11-1\ncategories:\n  - code\n  - analysis\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import our necessary libraries/modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Read in the dataset using Pandas\ndata = pd.read_csv('breast-cancer.csv')\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>...</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>...</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>...</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>...</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>...</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>...</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 32 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Extract the features that we are interested in for the model/analysis\nselected_features = [\n    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',\n    'symmetry_mean', 'fractal_dimension_mean'\n]\nX = data[selected_features]\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Standardize the features of interest to have mean = 0 and sd = 1\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00, ...,\n         2.53247522e+00,  2.21751501e+00,  2.25574689e+00],\n       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00, ...,\n         5.48144156e-01,  1.39236330e-03, -8.68652457e-01],\n       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00, ...,\n         2.03723076e+00,  9.39684817e-01, -3.98007910e-01],\n       ...,\n       [ 7.02284249e-01,  2.04557380e+00,  6.72675785e-01, ...,\n         1.05777359e-01, -8.09117071e-01, -8.95586935e-01],\n       [ 1.83834103e+00,  2.33645719e+00,  1.98252415e+00, ...,\n         2.65886573e+00,  2.13719425e+00,  1.04369542e+00],\n       [-1.80840125e+00,  1.22179204e+00, -1.81438851e+00, ...,\n        -1.26181958e+00, -8.20069901e-01, -5.61032377e-01]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Perform k-Means clustering using k = 3 clusters on the data\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=0)\nkmeans.fit(X_scaled)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nKMeans(n_clusters=3, random_state=0)\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Visualize the labels\ndata['Cluster'] = kmeans.labels_\ndata['Cluster'].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0    0\n1    1\n2    1\n3    0\n4    1\nName: Cluster, dtype: int32\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Use PCA to scale the data down to 2 dimensions on the standardized data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Plot the clusters from our k-Means clustering algorithm\nplt.figure(figsize=(8, 6))\nfor cluster in range(k):\n    plt.scatter(X_pca[data['Cluster'] == cluster, 0], X_pca[data['Cluster'] == cluster, 1], label=f'Cluster {cluster + 1}')\n\nplt.title('K-means Clustering (PCA)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=664 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}