{
  "hash": "f0e84428aba81635942328036a74fb61",
  "result": {
    "markdown": "---\ntitle: Linear and Nonlinear Regression\nauthor: Rithvik Guntor\ndate: 2023-11-1\ncategories:\n  - code\n  - analysis\n---\n\n**Lasso Regression**\n\nRegression analysis is a powerful statistical technique that plays a vital role in numerous fields, including healthcare. In the context of breast cancer research, regression can be used to visualize and analyze relationships between numeric variables in different studies. Lasso regression is a specific type of regression in which there is a penalizer in the linear regression form to help prevent overfitting. This method allows for better interpretability and more accurate predictions with regression methods. The code below performs Lasso regression using a breast cancer dataset with several variables. The code studies potential relationships between the age of a patient and the tumor size.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Load the dataset\ndata = pd.read_csv('gbsg.csv')\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>pid</th>\n      <th>age</th>\n      <th>meno</th>\n      <th>size</th>\n      <th>grade</th>\n      <th>nodes</th>\n      <th>pgr</th>\n      <th>er</th>\n      <th>hormon</th>\n      <th>rfstime</th>\n      <th>status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>132</td>\n      <td>49</td>\n      <td>0</td>\n      <td>18</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1838</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1575</td>\n      <td>55</td>\n      <td>1</td>\n      <td>20</td>\n      <td>3</td>\n      <td>16</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>403</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1140</td>\n      <td>56</td>\n      <td>1</td>\n      <td>40</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1603</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>769</td>\n      <td>45</td>\n      <td>0</td>\n      <td>25</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>177</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>130</td>\n      <td>65</td>\n      <td>1</td>\n      <td>30</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0</td>\n      <td>36</td>\n      <td>1</td>\n      <td>1855</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Define your feature matrix (X) and target variable (y)\nX = data[['age']]\ny = data['size']\n```\n:::\n\n\nWe want to split our data into training and testing data for model building/testing purposes.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Create a Lasso Regression model\nmodel = Lasso(alpha=1.0)\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Fit the Lasso model\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nLasso()\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Make predictions for the 'age' values\ny_pred = model.predict(X_test)\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([27.98806952, 29.37736335, 28.12038322, 29.4435202 , 28.38501062,\n       28.25269692, 30.30355924, 29.17889281, 28.58348116, 28.25269692,\n       29.17889281, 28.12038322, 28.51732432, 29.11273596, 29.24504965,\n       29.5758339 , 28.45116747, 28.71579486, 28.78195171, 28.31885377,\n       28.25269692, 29.7081476 , 29.90661815, 28.71579486, 28.64963801,\n       29.24504965, 28.18654007, 29.5758339 , 29.24504965, 29.11273596,\n       29.37736335, 29.5758339 , 28.91426541, 29.5758339 , 28.64963801,\n       28.78195171, 28.12038322, 29.7081476 , 28.51732432, 29.64199075,\n       29.5758339 , 28.71579486, 28.71579486, 28.25269692, 29.17889281,\n       28.71579486, 30.23740239, 29.64199075, 28.38501062, 29.17889281,\n       29.37736335, 29.77430445, 28.51732432, 28.58348116, 28.98042226,\n       29.8404613 , 28.58348116, 28.78195171, 29.7081476 , 29.50967705,\n       28.58348116, 28.91426541, 27.39265788, 28.25269692, 29.37736335,\n       28.58348116, 30.23740239, 28.78195171, 29.3112065 , 29.17889281,\n       29.04657911, 28.64963801, 28.78195171, 29.04657911, 29.04657911,\n       28.84810856, 28.05422637, 29.8404613 , 28.25269692, 29.4435202 ,\n       28.51732432, 29.4435202 , 29.5758339 , 28.51732432, 29.3112065 ,\n       29.64199075, 29.24504965, 28.71579486, 28.58348116, 29.37736335,\n       28.38501062, 30.03893184, 30.36971609, 28.25269692, 29.50967705,\n       29.64199075, 29.3112065 , 29.8404613 , 28.64963801, 29.3112065 ,\n       29.64199075, 29.5758339 , 30.17124554, 29.64199075, 29.5758339 ,\n       29.97277499, 29.50967705, 28.58348116, 29.50967705, 29.90661815,\n       29.3112065 , 29.11273596, 29.77430445, 28.78195171, 27.98806952,\n       28.84810856, 28.78195171, 29.24504965, 29.8404613 , 29.3112065 ,\n       28.71579486, 27.59112843, 28.84810856, 29.17889281, 28.71579486,\n       29.24504965, 28.78195171, 28.64963801, 29.3112065 , 30.23740239,\n       29.04657911, 29.04657911, 30.03893184, 29.77430445, 29.64199075,\n       27.72344213, 29.77430445, 28.45116747])\n```\n:::\n:::\n\n\nAfter making predictions using the test data, we want to verify the accuracy by computing the mean squared error and r-squared value.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nprint(\"Mean Squared Error: \", mse)\nprint(\"R-squared: \", r2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error:  193.888239405254\nR-squared:  -0.0036539165862292666\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Create a scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual', s=10)\nplt.plot(X_test, y_pred, color='red', label='Predicted', linewidth=2)\nplt.xlabel('Age')\nplt.ylabel('Tumor Size')\nplt.title('Age vs. Tumor Size (Lasso Regression)')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=593 height=449}\n:::\n:::\n\n\nBased on the output of our model and graph, it can be seen that there is a slightly negative relationship between the age of a patient and tumor size. This is indicated by r-squared value as well. Since we have an r-squared value of about -0.003, this means that not only is there a slightly negative relationship between age and tumor, but also a very weak/poor fit for the data, which can also be seen in the plot.\n\n**Random Forest Regression**\n\nFrom our previous Lasso regression model, the results were not as accurate as we were hoping for. However, we can try another type of regression model. This time, we will experiment with a nonlinear regression model known as RandomForest regression. It is a method to predict continuous values using a group of decision trees. Our linear approach using Lasso yielded a subpar result, so now let us experiment and examine how the RandomForest regressor compares.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Create a Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=4)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nRandomForestRegressor(random_state=4)\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\ny_pred\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([18.59770635, 32.03502357, 26.73878175, 28.46088973, 28.4890492 ,\n       20.64167063, 27.03738384, 27.49484865, 28.85783428, 20.64167063,\n       27.49484865, 26.73878175, 28.06173833, 28.02880346, 25.6333015 ,\n       35.35825081, 26.47801083, 29.64876783, 23.51566776, 25.58499862,\n       20.64167063, 25.72670516, 30.700719  , 29.64876783, 32.61029741,\n       25.6333015 , 26.64267238, 35.35825081, 25.6333015 , 28.02880346,\n       32.03502357, 35.35825081, 27.19539171, 35.35825081, 32.61029741,\n       23.51566776, 26.73878175, 25.72670516, 28.06173833, 30.93746124,\n       35.35825081, 29.64876783, 29.64876783, 20.64167063, 27.49484865,\n       29.64876783, 30.37329726, 30.93746124, 28.4890492 , 27.49484865,\n       32.03502357, 29.42056086, 28.06173833, 28.85783428, 27.40908095,\n       36.76734676, 28.85783428, 23.51566776, 25.72670516, 33.36409916,\n       28.85783428, 27.19539171, 17.48      , 20.64167063, 32.03502357,\n       28.85783428, 30.37329726, 23.51566776, 30.08202232, 27.49484865,\n       25.00801156, 32.61029741, 23.51566776, 25.00801156, 25.00801156,\n       36.19086049, 28.62259257, 36.76734676, 20.64167063, 28.46088973,\n       28.06173833, 28.46088973, 35.35825081, 28.06173833, 30.08202232,\n       30.93746124, 25.6333015 , 29.64876783, 28.85783428, 32.03502357,\n       28.4890492 , 30.57222763, 31.88861616, 20.64167063, 33.36409916,\n       30.93746124, 30.08202232, 36.76734676, 32.61029741, 30.08202232,\n       30.93746124, 35.35825081, 31.35149995, 30.93746124, 35.35825081,\n       31.02165705, 33.36409916, 28.85783428, 33.36409916, 30.700719  ,\n       30.08202232, 28.02880346, 29.42056086, 23.51566776, 18.59770635,\n       36.19086049, 23.51566776, 25.6333015 , 36.76734676, 30.08202232,\n       29.64876783, 38.62      , 36.19086049, 27.49484865, 29.64876783,\n       25.6333015 , 23.51566776, 32.61029741, 30.08202232, 30.37329726,\n       25.00801156, 25.00801156, 30.57222763, 29.42056086, 30.93746124,\n       42.43083333, 29.42056086, 26.47801083])\n```\n:::\n:::\n\n\nAfter making predictions using the test data, we want to verify the accuracy by computing the mean squared error and r-squared value.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 234.9018625033204\nR-squared: -0.2159591269591401\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Plot our data and line smoothly\nplt.scatter(X_test, y_test, color='b', label='Actual')\nX_test_sorted = np.sort(X_test, axis=0)\ny_pred_sorted = rf_model.predict(X_test_sorted)\nplt.plot(X_test_sorted, y_pred_sorted, color='r', label='Predicted')\n\nplt.xlabel('Age')\nplt.ylabel('Tumor Size')\nplt.title('Random Forest Regression: Age vs. Tumor Size')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/rithvikguntor/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:\n\nX does not have valid feature names, but RandomForestRegressor was fitted with feature names\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-2.png){width=593 height=449}\n:::\n:::\n\n\nBased on our results from the code output and graph above, it can be seen that with an r-squared value of about -0.22, we have a slightly better fit compared to the Lasso regression approach. However, this model is still a poor fit in the larger scope. The data itself appears to be fairly scattered and does not immediately appear to be linear or have any sort of clear pattern/relationship. Both models appear to be in the negative direction. We have a negative and weak relationship between the age and tumor size of the patient. Despite a slight improvement from the Lasso model, both models are ultimately a poor fit for the data. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}