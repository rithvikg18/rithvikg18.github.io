[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Breast Cancer Prediction & Analysis",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRithvik Guntor\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRithvik Guntor\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRithvik Guntor\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRithvik Guntor\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nRithvik Guntor\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Clustering",
    "section": "",
    "text": "# Import our necessary libraries/modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# Read in the dataset using Pandas\ndata = pd.read_csv('breast-cancer.csv')\ndata.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n# Extract the features that we are interested in for the model/analysis\nselected_features = [\n    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',\n    'symmetry_mean', 'fractal_dimension_mean'\n]\nX = data[selected_features]\nX.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n\n\n\n\n\n\n\n\n# Standardize the features of interest to have mean = 0 and sd = 1\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00, ...,\n         2.53247522e+00,  2.21751501e+00,  2.25574689e+00],\n       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00, ...,\n         5.48144156e-01,  1.39236330e-03, -8.68652457e-01],\n       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00, ...,\n         2.03723076e+00,  9.39684817e-01, -3.98007910e-01],\n       ...,\n       [ 7.02284249e-01,  2.04557380e+00,  6.72675785e-01, ...,\n         1.05777359e-01, -8.09117071e-01, -8.95586935e-01],\n       [ 1.83834103e+00,  2.33645719e+00,  1.98252415e+00, ...,\n         2.65886573e+00,  2.13719425e+00,  1.04369542e+00],\n       [-1.80840125e+00,  1.22179204e+00, -1.81438851e+00, ...,\n        -1.26181958e+00, -8.20069901e-01, -5.61032377e-01]])\n\n\n\n# Perform k-Means clustering using k = 3 clusters on the data\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=0)\nkmeans.fit(X_scaled)\n\nKMeans(n_clusters=3, random_state=0)\n\n\n\n# Visualize the labels\ndata['Cluster'] = kmeans.labels_\ndata['Cluster'].head()\n\n0    0\n1    1\n2    1\n3    0\n4    1\nName: Cluster, dtype: int32\n\n\n\n# Use PCA to scale the data down to 2 dimensions on the standardized data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n\n# Plot the clusters from our k-Means clustering algorithm\nplt.figure(figsize=(8, 6))\nfor cluster in range(k):\n    plt.scatter(X_pca[data['Cluster'] == cluster, 0], X_pca[data['Cluster'] == cluster, 1], label=f'Cluster {cluster + 1}')\n\nplt.title('K-means Clustering (PCA)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Blog Post Code & Analysis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Rithvik Guntor. I graduated from Virginia Tech with a Bachelor’s in Computational Modeling Data Analytics in December 2022. I am a current VT CS graduate student interested in the fields of software engineering and machine learning. In order to further explore the field of ML, I wrote a few blogs to dive deeper into these concepts through Breast Cancer data."
  },
  {
    "objectID": "posts/Blog Post 2/index.html",
    "href": "posts/Blog Post 2/index.html",
    "title": "Clustering",
    "section": "",
    "text": "K-Means Clustering\nClustering is a fundamental technique in data analysis and machine learning that involves grouping similar data points together. These techniques are often used in breast cancer studies and analysis to identify groups and subgroups based on various characteristics among breast cancer patients. Visualizing this information in clusters can assist health experts in identifying potential groups of patients at risk of breast cancer. One particular clustering algorithm is K-means. This clustering algorithm helps to identify similar characteristics among various data points by selecting a K-value to represent how many clusters you wish to utilize in a study. The code below shows how K-means works using a breast cancer dataset.\nCode & Visuals\n\n# Import our necessary libraries/modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# Read in the dataset using Pandas\ndata = pd.read_csv('breast-cancer.csv')\ndata.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n# Extract the features that we are interested in for the model/analysis\nselected_features = [\n    'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean',\n    'symmetry_mean', 'fractal_dimension_mean'\n]\nX = data[selected_features]\nX.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n\n\n\n\n\n\n\nWe standardize the data so that there isn’t a particular feature that exhibits more influence over the others.\n\n# Standardize the features of interest to have mean = 0 and sd = 1\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_scaled\n\narray([[ 1.09706398e+00, -2.07333501e+00,  1.26993369e+00, ...,\n         2.53247522e+00,  2.21751501e+00,  2.25574689e+00],\n       [ 1.82982061e+00, -3.53632408e-01,  1.68595471e+00, ...,\n         5.48144156e-01,  1.39236330e-03, -8.68652457e-01],\n       [ 1.57988811e+00,  4.56186952e-01,  1.56650313e+00, ...,\n         2.03723076e+00,  9.39684817e-01, -3.98007910e-01],\n       ...,\n       [ 7.02284249e-01,  2.04557380e+00,  6.72675785e-01, ...,\n         1.05777359e-01, -8.09117071e-01, -8.95586935e-01],\n       [ 1.83834103e+00,  2.33645719e+00,  1.98252415e+00, ...,\n         2.65886573e+00,  2.13719425e+00,  1.04369542e+00],\n       [-1.80840125e+00,  1.22179204e+00, -1.81438851e+00, ...,\n        -1.26181958e+00, -8.20069901e-01, -5.61032377e-01]])\n\n\n\n# Perform k-Means clustering using k = 3 clusters on the data\nk = 3\nkmeans = KMeans(n_clusters=k, random_state=0)\nkmeans.fit(X_scaled)\n\nKMeans(n_clusters=3, random_state=0)\n\n\n\n# Visualize the labels\ndata['Cluster'] = kmeans.labels_\ndata['Cluster'].head()\n\n0    0\n1    1\n2    1\n3    0\n4    1\nName: Cluster, dtype: int32\n\n\nWe can scale the data using PCA to simplify complex data with many features as well as for visualization purposes.\n\n# Use PCA to scale the data down to 2 dimensions on the standardized data\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n\n# Plot the clusters from our k-Means clustering algorithm\nplt.figure(figsize=(8, 6))\nfor cluster in range(k):\n    plt.scatter(X_pca[data['Cluster'] == cluster, 0], X_pca[data['Cluster'] == cluster, 1], label=f'Cluster {cluster + 1}')\n\nplt.title('K-means Clustering (PCA)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n\n\n\n\nAs it can be seen in the code and the visualization, we are utilizing K = 3 clusters and 2 PCA components for this study. We can see that there are 3 distinct groups in the cluster shown, with each data point in each cluster sharing similar characteristics with each other. The data in our study was first standardized so that no single feature dominates the analysis; equal importance is given to all features in the data. Next, Principle-Component-Analysis (PCA) is performed on the standardized data points to scale the data down to 2 dimensions.\nThis technique is often used so that complex data with many features can be simplified for understanding, visualization, and for interpretation of the data. In this study, PCA was performed to visualize our data clusters using 2 dimensions, which are our axes.The K-means algorithm was ultimately able to place various data points into groups based on their characteristics in a clean and organized manner, as per the visualization.\nThe DBSCAN Clustering Algorithm Comparison\nK-Means is a powerful algorithm for grouping data. The algorithm however has various limitations. It is sensitive to noisy data as it assigns each point to the nearest centroid; outliers can significantly influence the centroids’ positions. There are several other clustering algorithms that function similarly as well. Another particular clustering algorithm is Density-Based Spatial Clustering of Applications with Noise (DBSCAN). DBSCAN groups data points based on their density. It forms clusters around areas with high data density while marking outliers as noise. It’s effective for discovering clusters of erratic/unpredictable shapes.\nDBSCAN is different from K-Means because it doesn’t need you to guess how many groups there are in the data. Instead, it figures out the groups based on where the data points are huddled together. DBSCAN can find groups of various shapes and sizes, and it’s not too bothered by some noisy or scattered data points. K-Means clustering, on the other hand, needs you to tell it how many groups you think there are in advance and assumes the groups are all about the same size and shape. DBSCAN is often more accurate than K-Means when dealing with real-world data because it can handle different shapes of groups and noisy data more effectively."
  },
  {
    "objectID": "posts/Blog Post 3/index.html",
    "href": "posts/Blog Post 3/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Lasso Regression\nRegression analysis is a powerful statistical technique that plays a vital role in numerous fields, including healthcare. In the context of breast cancer research, regression can be used to visualize and analyze relationships between numeric variables in different studies. Lasso regression is a specific type of regression in which there is a penalizer in the linear regression form to help prevent overfitting. This method allows for better interpretability and more accurate predictions with regression methods. The code below performs Lasso regression using a breast cancer dataset with several variables. The code studies potential relationships between the age of a patient and the tumor size.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Load the dataset\ndata = pd.read_csv('gbsg.csv')\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\npid\nage\nmeno\nsize\ngrade\nnodes\npgr\ner\nhormon\nrfstime\nstatus\n\n\n\n\n0\n1\n132\n49\n0\n18\n2\n2\n0\n0\n0\n1838\n0\n\n\n1\n2\n1575\n55\n1\n20\n3\n16\n0\n0\n0\n403\n1\n\n\n2\n3\n1140\n56\n1\n40\n3\n3\n0\n0\n0\n1603\n0\n\n\n3\n4\n769\n45\n0\n25\n3\n1\n0\n4\n0\n177\n0\n\n\n4\n5\n130\n65\n1\n30\n2\n5\n0\n36\n1\n1855\n0\n\n\n\n\n\n\n\n\n# Define your feature matrix (X) and target variable (y)\nX = data[['age']]\ny = data['size']\n\nWe want to split our data into training and testing data for model building/testing purposes.\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n\n\n# Create a Lasso Regression model\nmodel = Lasso(alpha=1.0)\n\n\n# Fit the Lasso model\nmodel.fit(X_train, y_train)\n\nLasso()\n\n\n\n# Make predictions for the 'age' values\ny_pred = model.predict(X_test)\ny_pred\n\narray([27.98806952, 29.37736335, 28.12038322, 29.4435202 , 28.38501062,\n       28.25269692, 30.30355924, 29.17889281, 28.58348116, 28.25269692,\n       29.17889281, 28.12038322, 28.51732432, 29.11273596, 29.24504965,\n       29.5758339 , 28.45116747, 28.71579486, 28.78195171, 28.31885377,\n       28.25269692, 29.7081476 , 29.90661815, 28.71579486, 28.64963801,\n       29.24504965, 28.18654007, 29.5758339 , 29.24504965, 29.11273596,\n       29.37736335, 29.5758339 , 28.91426541, 29.5758339 , 28.64963801,\n       28.78195171, 28.12038322, 29.7081476 , 28.51732432, 29.64199075,\n       29.5758339 , 28.71579486, 28.71579486, 28.25269692, 29.17889281,\n       28.71579486, 30.23740239, 29.64199075, 28.38501062, 29.17889281,\n       29.37736335, 29.77430445, 28.51732432, 28.58348116, 28.98042226,\n       29.8404613 , 28.58348116, 28.78195171, 29.7081476 , 29.50967705,\n       28.58348116, 28.91426541, 27.39265788, 28.25269692, 29.37736335,\n       28.58348116, 30.23740239, 28.78195171, 29.3112065 , 29.17889281,\n       29.04657911, 28.64963801, 28.78195171, 29.04657911, 29.04657911,\n       28.84810856, 28.05422637, 29.8404613 , 28.25269692, 29.4435202 ,\n       28.51732432, 29.4435202 , 29.5758339 , 28.51732432, 29.3112065 ,\n       29.64199075, 29.24504965, 28.71579486, 28.58348116, 29.37736335,\n       28.38501062, 30.03893184, 30.36971609, 28.25269692, 29.50967705,\n       29.64199075, 29.3112065 , 29.8404613 , 28.64963801, 29.3112065 ,\n       29.64199075, 29.5758339 , 30.17124554, 29.64199075, 29.5758339 ,\n       29.97277499, 29.50967705, 28.58348116, 29.50967705, 29.90661815,\n       29.3112065 , 29.11273596, 29.77430445, 28.78195171, 27.98806952,\n       28.84810856, 28.78195171, 29.24504965, 29.8404613 , 29.3112065 ,\n       28.71579486, 27.59112843, 28.84810856, 29.17889281, 28.71579486,\n       29.24504965, 28.78195171, 28.64963801, 29.3112065 , 30.23740239,\n       29.04657911, 29.04657911, 30.03893184, 29.77430445, 29.64199075,\n       27.72344213, 29.77430445, 28.45116747])\n\n\nAfter making predictions using the test data, we want to verify the accuracy by computing the mean squared error and r-squared value.\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(\"Mean Squared Error: \", mse)\nprint(\"R-squared: \", r2)\n\nMean Squared Error:  193.888239405254\nR-squared:  -0.0036539165862292666\n\n\n\n# Create a scatter plot\nplt.scatter(X_test, y_test, color='blue', label='Actual', s=10)\nplt.plot(X_test, y_pred, color='red', label='Predicted', linewidth=2)\nplt.xlabel('Age')\nplt.ylabel('Tumor Size')\nplt.title('Age vs. Tumor Size (Lasso Regression)')\nplt.legend()\nplt.show()\n\n\n\n\nBased on the output of our model and graph, it can be seen that there is a slightly negative relationship between the age of a patient and tumor size. This is indicated by r-squared value as well. Since we have an r-squared value of about -0.003, this means that not only is there a slightly negative relationship between age and tumor, but also a very weak/poor fit for the data, which can also be seen in the plot.\nRandom Forest Regression\nFrom our previous Lasso regression model, the results were not as accurate as we were hoping for. However, we can try another type of regression model. This time, we will experiment with a nonlinear regression model known as RandomForest regression. It is a method to predict continuous values using a group of decision trees. Our linear approach using Lasso yielded a subpar result, so now let us experiment and examine how the RandomForest regressor compares.\n\n# Create a Random Forest Regressor\nrf_model = RandomForestRegressor(n_estimators=100, random_state=4)\n\n\n# Fit the model to the training data\nrf_model.fit(X_train, y_train)\n\nRandomForestRegressor(random_state=4)\n\n\n\n# Make predictions on the test set\ny_pred = rf_model.predict(X_test)\ny_pred\n\narray([18.59770635, 32.03502357, 26.73878175, 28.46088973, 28.4890492 ,\n       20.64167063, 27.03738384, 27.49484865, 28.85783428, 20.64167063,\n       27.49484865, 26.73878175, 28.06173833, 28.02880346, 25.6333015 ,\n       35.35825081, 26.47801083, 29.64876783, 23.51566776, 25.58499862,\n       20.64167063, 25.72670516, 30.700719  , 29.64876783, 32.61029741,\n       25.6333015 , 26.64267238, 35.35825081, 25.6333015 , 28.02880346,\n       32.03502357, 35.35825081, 27.19539171, 35.35825081, 32.61029741,\n       23.51566776, 26.73878175, 25.72670516, 28.06173833, 30.93746124,\n       35.35825081, 29.64876783, 29.64876783, 20.64167063, 27.49484865,\n       29.64876783, 30.37329726, 30.93746124, 28.4890492 , 27.49484865,\n       32.03502357, 29.42056086, 28.06173833, 28.85783428, 27.40908095,\n       36.76734676, 28.85783428, 23.51566776, 25.72670516, 33.36409916,\n       28.85783428, 27.19539171, 17.48      , 20.64167063, 32.03502357,\n       28.85783428, 30.37329726, 23.51566776, 30.08202232, 27.49484865,\n       25.00801156, 32.61029741, 23.51566776, 25.00801156, 25.00801156,\n       36.19086049, 28.62259257, 36.76734676, 20.64167063, 28.46088973,\n       28.06173833, 28.46088973, 35.35825081, 28.06173833, 30.08202232,\n       30.93746124, 25.6333015 , 29.64876783, 28.85783428, 32.03502357,\n       28.4890492 , 30.57222763, 31.88861616, 20.64167063, 33.36409916,\n       30.93746124, 30.08202232, 36.76734676, 32.61029741, 30.08202232,\n       30.93746124, 35.35825081, 31.35149995, 30.93746124, 35.35825081,\n       31.02165705, 33.36409916, 28.85783428, 33.36409916, 30.700719  ,\n       30.08202232, 28.02880346, 29.42056086, 23.51566776, 18.59770635,\n       36.19086049, 23.51566776, 25.6333015 , 36.76734676, 30.08202232,\n       29.64876783, 38.62      , 36.19086049, 27.49484865, 29.64876783,\n       25.6333015 , 23.51566776, 32.61029741, 30.08202232, 30.37329726,\n       25.00801156, 25.00801156, 30.57222763, 29.42056086, 30.93746124,\n       42.43083333, 29.42056086, 26.47801083])\n\n\nAfter making predictions using the test data, we want to verify the accuracy by computing the mean squared error and r-squared value.\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2)\n\nMean Squared Error: 234.9018625033204\nR-squared: -0.2159591269591401\n\n\n\n# Plot our data and line smoothly\nplt.scatter(X_test, y_test, color='b', label='Actual')\nX_test_sorted = np.sort(X_test, axis=0)\ny_pred_sorted = rf_model.predict(X_test_sorted)\nplt.plot(X_test_sorted, y_pred_sorted, color='r', label='Predicted')\n\nplt.xlabel('Age')\nplt.ylabel('Tumor Size')\nplt.title('Random Forest Regression: Age vs. Tumor Size')\nplt.legend()\nplt.show()\n\n/Users/rithvikguntor/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:\n\nX does not have valid feature names, but RandomForestRegressor was fitted with feature names\n\n\n\n\n\n\nBased on our results from the code output and graph above, it can be seen that with an r-squared value of about -0.22, we have a slightly better fit compared to the Lasso regression approach. However, this model is still a poor fit in the larger scope. The data itself appears to be fairly scattered and does not immediately appear to be linear or have any sort of clear pattern/relationship. Both models appear to be in the negative direction. We have a negative and weak relationship between the age and tumor size of the patient. Despite a slight improvement from the Lasso model, both models are ultimately a poor fit for the data."
  },
  {
    "objectID": "posts/Blog Post 1/index.html",
    "href": "posts/Blog Post 1/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "Probability Theory and Random Variables are an important component of statistical analysis in various studies. Probability theory serves as a framework for quantifying the likelihood of various outcomes in many statistical problems. In particular, probability theory and random variables have proven invaluable in the domain of breast cancer research. They enable researchers to model and analyze complex data, and facilitate a deeper understanding of the disease’s prevalence, risk factors, and treatment outcomes, ultimately contributing to more effective diagnosis and treatment strategies. It’s application and connection in breast cancer research allows us to better understand uncertainty in breast cancer cases as well as many other real-world applications. The below code provides a closer look at probability theory and random variable study using a breast cancer dataset.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, lognorm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\n\n\n# Load your breast cancer dataset\ndata = pd.read_csv('Breast_Cancer.csv')\ndata.head()\n\n\n\n\n\n\n\n\nAge\nRace\nMarital Status\nT Stage\nN Stage\n6th Stage\ndifferentiate\nGrade\nA Stage\nTumor Size\nEstrogen Status\nProgesterone Status\nRegional Node Examined\nReginol Node Positive\nSurvival Months\nStatus\n\n\n\n\n0\n68\nWhite\nMarried\nT1\nN1\nIIA\nPoorly differentiated\n3\nRegional\n4\nPositive\nPositive\n24\n1\n60\nAlive\n\n\n1\n50\nWhite\nMarried\nT2\nN2\nIIIA\nModerately differentiated\n2\nRegional\n35\nPositive\nPositive\n14\n5\n62\nAlive\n\n\n2\n58\nWhite\nDivorced\nT3\nN3\nIIIC\nModerately differentiated\n2\nRegional\n63\nPositive\nPositive\n14\n7\n75\nAlive\n\n\n3\n58\nWhite\nMarried\nT1\nN1\nIIA\nPoorly differentiated\n3\nRegional\n18\nPositive\nPositive\n2\n1\n84\nAlive\n\n\n4\n47\nWhite\nMarried\nT2\nN1\nIIB\nPoorly differentiated\n3\nRegional\n41\nPositive\nPositive\n3\n1\n50\nAlive\n\n\n\n\n\n\n\nWe specifically want to examine the patient age and tumor size varibales for this study.\n\n# Select the relevant features: Age and Tumor Size\nX = data[['Age', 'Tumor Size']]\nX\n\n\n\n\n\n\n\n\nAge\nTumor Size\n\n\n\n\n0\n68\n4\n\n\n1\n50\n35\n\n\n2\n58\n63\n\n\n3\n58\n18\n\n\n4\n47\n41\n\n\n...\n...\n...\n\n\n4019\n62\n9\n\n\n4020\n56\n46\n\n\n4021\n68\n22\n\n\n4022\n58\n44\n\n\n4023\n46\n30\n\n\n\n\n4024 rows × 2 columns\n\n\n\nWe standardize the data to mitigate the effect of outliers and simplify interpretability.\n\n# Step 1: Standardize the data\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\n\n\n# Compute mean/standard deviation for the standardized data\nfor feature in ['Age', 'Tumor Size']:\n    mean = X_std[:, X.columns.get_loc(feature)].mean()\n    std = X_std[:, X.columns.get_loc(feature)].std()\n    print(f\"{feature}: Mean = {mean}, Standard Deviation = {std}\")\n\nAge: Mean = -4.855846230964899e-17, Standard Deviation = 1.0\nTumor Size: Mean = 5.120710570835712e-17, Standard Deviation = 1.0\n\n\n\n# Plot Histograms and Fit Gaussian Distributions\nplt.figure(figsize=(12, 5))\nfor i, feature in enumerate(['Age', 'Tumor Size'], start=1):\n    plt.subplot(1, 2, i)\n    plt.hist(X_std[:, X.columns.get_loc(feature)], bins=20, density=True, alpha=0.6, color='b', label=feature)\n    x = np.linspace(X_std[:, X.columns.get_loc(feature)].min(), X_std[:, X.columns.get_loc(feature)].max(), 100)\n    \n    # Fit a Gaussian (Normal) distribution\n    mu, std = norm.fit(X_std[:, X.columns.get_loc(feature)])\n    p = norm.pdf(x, mu, std)\n    plt.plot(x, p, 'k', linewidth=2, label=f'Normal Fit:\\n$\\mu$={mu:.2f}, $\\sigma$={std:.2f}')\n    \n    plt.xlabel(feature + ' (Standardized)')\n    plt.ylabel('Probability')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n# Calculate mean for Age and Tumor Size\nmeans = X.mean()\nmeans\n\nAge           53.972167\nTumor Size    30.473658\ndtype: float64\n\n\n\n# Calculate variance Age and Tumor Size\nvariances = X.var()\nvariances\n\nAge            80.337778\nTumor Size    446.041563\ndtype: float64\n\n\n\n# Calculate covariance matrix for Age and Tumor Size\ncovariance_matrix = X.cov()\ncovariance_matrix\n\n\n\n\n\n\n\n\nAge\nTumor Size\n\n\n\n\nAge\n80.337778\n-14.616692\n\n\nTumor Size\n-14.616692\n446.041563\n\n\n\n\n\n\n\nThe results above show that the age variable has a higher mean than the tumor size variable. However, it is interesting that the tumor size appears to have a significantly larger variance compared to the age. It can be seen from the visualizations above that we have fitted normal Gaussian distributions. The standardized tumor size histogram/curve appear to be slightly skewed to the right compared to the standardized age histogram/curve. Though the tumor size histogram is standardized, the shape of the curve may be a result of the significant variance in the tumor size variable.\nGaussian Mixture Model\nAnother technique that is often applicable to probability theory and random variable studies is the Gaussian Mixture Model method. This method is used to represent complex variable distributions. It can also be used in anomaly/outlier detection as well (see the anomaly/outlier detection post). GMMs are used to estimate the probability density of a continuous random variable. They represent the probability distribution as a weighted sum of multiple Gaussian distributions. The GMM can provide a more flexible representation of complex data distributions as opposed to a single Gaussian distribution. The code below explores the variables from the same dataset using the GMM technique.\n\n# Use the Gaussian Mixture Model Approach\ngm = GaussianMixture(n_components=3, n_init=10, random_state=42)\ngm.fit(X)\n\nGaussianMixture(n_components=3, n_init=10, random_state=42)\n\n\n\n# Computes the weights\ngm.weights_\n\narray([0.58662902, 0.13513493, 0.27823604])\n\n\n\n# Computes the means\ngm.means_\n\narray([[54.8125781 , 18.22267396],\n       [52.17021474, 69.50342509],\n       [53.07543614, 37.34731049]])\n\n\n\n# Computes the covariances\ngm.covariances_\n\narray([[[ 76.68503418,  -1.77744933],\n        [ -1.77744933,  45.93142242]],\n\n       [[ 85.62276729,   4.36998368],\n        [  4.36998368, 555.47154068]],\n\n       [[ 81.53029285,  11.13430287],\n        [ 11.13430287, 132.53955524]]])\n\n\n\n# Checks if algorithm converged\ngm.converged_\n\nTrue\n\n\n\n# Checks the number of iterations algorithm took to converge\ngm.n_iter_\n\n6\n\n\n\n# Predicts which cluster each instance belongs to\ngm.predict(X)\n\narray([0, 2, 1, ..., 0, 2, 2])\n\n\n\n# Predicts probabilities that instance came from cluster\ngm.predict_proba(X).round(3)\n\narray([[0.978, 0.007, 0.015],\n       [0.132, 0.067, 0.802],\n       [0.   , 0.695, 0.305],\n       ...,\n       [0.919, 0.006, 0.075],\n       [0.003, 0.121, 0.876],\n       [0.432, 0.039, 0.529]])\n\n\n\n# Estimate the log of the PDF at any location \ngm.score_samples(X).round(2)\n\narray([-9.68, -7.61, -9.1 , ..., -7.69, -7.91, -7.58])\n\n\nThe code above uses the Gaussian Mixed Model method to estimate the logarithm of the probability density function of the data points in the variable ‘X’. The GMM approach is a powerful tool in probability theory and random variable studies. It’s applications in real-world situations like breast cancer studies make it invaluable throughout the industry and research. GMM enables us to model various data distributions to gain a better understanding of probability distribution functions, anomaly detection, and random variables as a whole. This versatile tool continues to allow statisticians and researchers to capture various components of complex data distributions."
  },
  {
    "objectID": "posts/Blog Post 4/index.html",
    "href": "posts/Blog Post 4/index.html",
    "title": "Classification",
    "section": "",
    "text": "Random Forest Classification\nClassification is state-of-the-art supervised machine learning algorithm that is often used to categorize/classify data into distinct classes based on their attributes. These algorithms require labeled data for training purposes. This subset of machine learning is often used in healthcare, and we will take a dive into how this concept can be applied in breast cancer research/modeling. There are various classification algorithms that can be used depending on the problem at hand, however the one we will take a closer look at for now is Random Forest classification. Random Forest classification makes predictions about categories or classes for data points using a collection of decision trees. These trees collaborate to develop increasingly reliable results using the data features. The code below explores how Random Forest uses breast cancer data to classify a patient tumor as benign or malignant.\n\n# We import our necessary libraries/modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nWe are using Python’s built-in breast cancer dataset for this study.\n\n# Load in sci-kit learn's breast cancer dataset\ndata = load_breast_cancer()\ndata\n\n{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n         1.189e-01],\n        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n         8.902e-02],\n        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n         8.758e-02],\n        ...,\n        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n         7.820e-02],\n        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n         1.240e-01],\n        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n         7.039e-02]]),\n 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n 'frame': None,\n 'target_names': array(['malignant', 'benign'], dtype='&lt;U9'),\n 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n        'mean smoothness', 'mean compactness', 'mean concavity',\n        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n        'radius error', 'texture error', 'perimeter error', 'area error',\n        'smoothness error', 'compactness error', 'concavity error',\n        'concave points error', 'symmetry error',\n        'fractal dimension error', 'worst radius', 'worst texture',\n        'worst perimeter', 'worst area', 'worst smoothness',\n        'worst compactness', 'worst concavity', 'worst concave points',\n        'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23'),\n 'filename': 'breast_cancer.csv',\n 'data_module': 'sklearn.datasets.data'}\n\n\n\n# Split up our X-value data\nX = pd.DataFrame(data.data, columns = data.feature_names)\nX\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\n\n# Split up our y-value data\ny = pd.Series(data.target)\ny\n\n0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n564    0\n565    0\n566    0\n567    0\n568    1\nLength: 569, dtype: int64\n\n\n\n# Split our data into training and testing data to develop our model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\n# Initialize the model\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n# Fit our model usin the training data\nclf.fit(X_train, y_train)\n\nRandomForestClassifier(random_state=42)\n\n\n\n# Make predictions using the X-testing data\ny_pred = clf.predict(X_test)\ny_pred\n\narray([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n\n\nAfter making predictions with the model, we want to verify its accuracy by comparing the predicted results with the actual test results.\n\n# Compute the accuracy based on our calculated y-data and the y-test data\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9707602339181286\n\n\nWe can visualize the effect of each feature to better understand the most significant features in the Random Forest study.\n\n# Sort feature importances for visualization purposes\nfeature_importances = clf.feature_importances_\nsorted_indices = np.argsort(feature_importances)[::-1]\nsorted_features = X.columns[sorted_indices]\n\n\n# Plot the sorted feature importances in descending order to view the most significant features\nplt.figure(figsize=(12, 6))\nplt.title(\"Feature Importances\")\nplt.bar(range(X.shape[1]), feature_importances[sorted_indices], align=\"center\")\nplt.xticks(range(X.shape[1]), sorted_features, rotation=90)\nplt.tight_layout()\nplt.show()\n\n\n\n\nWe can see that from the code output above, the model appears to be fairly accurate with an accuracy score of 0.97. The model appears to correctly classify the data for about 97 percent of the samples, and misclassifying 3 percent. These results indicate that the Random Forest model performs well on the dataset by correctly predicting the majority of the sample data. Let’s take a closer look at which features in the model likely contributed to these results. In the above visual, we graphed the importance of each of the features to see which ones were the most significant in the model. It can be seen that “mean concave points” and “worst concave points” appear to be the most influential features in the model.\nSupport Vector Machine Classification\nAs it was seen that the results that were yielded from the Random Forest model were accurate, we can experiment with another classification model to see if we can achieve even more accurate results. The Support Vector Machine (SVM) algorithm is a classification algorithm in which a line or boundary is draw to separate different types of data points in a way that maximizes the gap. The code below performs the same analysis using the SVM algorithm with the goal of more accurate results.\n\n# Import the SVM fro sci-kit learn and initialize a model\nfrom sklearn.svm import SVC\n\nsvm_classifier = SVC(kernel='linear', random_state=42)\n\n\n# Fit the model using the training data\nsvm_classifier.fit(X_train, y_train)\n\nSVC(kernel='linear', random_state=42)\n\n\n\n# Make predictions with the newly fitted model using the X-test data\ny_pred = svm_classifier.predict(X_test)\ny_pred\n\narray([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])\n\n\nAfter making predictions with the model, we want to verify its accuracy by comparing the predicted results with the actual test results.\n\n# Compute the accuracy based on our calculated y-data and the y-test data\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\nAccuracy: 0.96\n\n\n\n# Compute/generate the feature importances\nfeature_importances = np.abs(svm_classifier.coef_)[0]\nfeature_importances\n\narray([1.18459174e+00, 1.57213948e-01, 2.53328286e-01, 6.00083096e-03,\n       2.06672071e-01, 2.73003057e-01, 6.09220472e-01, 3.70822278e-01,\n       3.68946148e-01, 4.66278921e-02, 2.02377021e-01, 1.80355325e+00,\n       3.92315274e-01, 1.11576684e-01, 2.74209471e-02, 6.06321641e-02,\n       4.50149140e-04, 3.53882712e-02, 5.92002603e-02, 1.13060266e-02,\n       3.83819674e-01, 3.76288719e-01, 3.02801050e-03, 7.14182793e-04,\n       4.05825264e-01, 8.11785632e-01, 1.48890783e+00, 5.78520618e-01,\n       1.24749021e+00, 1.10225090e-01])\n\n\n\n# Sort the feature importances in descending order\ndf = pd.DataFrame({'names': data.feature_names, 'importance':feature_importances})\ndf_sorted = df.sort_values(by='importance', ascending=False)\ndf_sorted\n\n\n\n\n\n\n\n\nnames\nimportance\n\n\n\n\n11\ntexture error\n1.803553\n\n\n26\nworst concavity\n1.488908\n\n\n28\nworst symmetry\n1.247490\n\n\n0\nmean radius\n1.184592\n\n\n25\nworst compactness\n0.811786\n\n\n6\nmean concavity\n0.609220\n\n\n27\nworst concave points\n0.578521\n\n\n24\nworst smoothness\n0.405825\n\n\n12\nperimeter error\n0.392315\n\n\n20\nworst radius\n0.383820\n\n\n21\nworst texture\n0.376289\n\n\n7\nmean concave points\n0.370822\n\n\n8\nmean symmetry\n0.368946\n\n\n5\nmean compactness\n0.273003\n\n\n2\nmean perimeter\n0.253328\n\n\n4\nmean smoothness\n0.206672\n\n\n10\nradius error\n0.202377\n\n\n1\nmean texture\n0.157214\n\n\n13\narea error\n0.111577\n\n\n29\nworst fractal dimension\n0.110225\n\n\n15\ncompactness error\n0.060632\n\n\n18\nsymmetry error\n0.059200\n\n\n9\nmean fractal dimension\n0.046628\n\n\n17\nconcave points error\n0.035388\n\n\n14\nsmoothness error\n0.027421\n\n\n19\nfractal dimension error\n0.011306\n\n\n3\nmean area\n0.006001\n\n\n22\nworst perimeter\n0.003028\n\n\n23\nworst area\n0.000714\n\n\n16\nconcavity error\n0.000450\n\n\n\n\n\n\n\nWe can visualize the effect of each feature to better understand the most significant features in the SVM study.\n\n# Plot the sorted feature importances in descending order to view the most significant features\nplt.figure(figsize=(10, 6))\nplt.bar(df_sorted.names, df_sorted.importance, color='skyblue')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances (SVM)')\nplt.xticks(range(X.shape[1]), sorted_features, rotation=90)\nplt.show()\n\n\n\n\nIt can be seen that from the results above, we have an accuracy score of about 0.96. The Random Forest model appears to be a better fit for this data as it yielded a slightly higher accuracy score of 0.97. However, even though that the Random Forest model yielded higher accuracy score, the SVM model appears to still be fairly accurate as it correctly predicts 96 percent of the sample data. Additionally, it can be seen in the above visualization that the “mean concave points” and “worst concave points” are the most influential features in our SVM model, which was the same case with our Random Forest model. These features appear to be the most significant features in tumor classification in this study. Ultimately, both the Random Forest and SVM models are fairly accurate with tumor prediction, however the Random Forest model appeared to be a slightly more accurate predictor in this study."
  },
  {
    "objectID": "posts/Blog Post 5/index.html",
    "href": "posts/Blog Post 5/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Isolation Forest\nAnomaly and outlier detection is a statistical technique used to identify data points that deviate significantly from the rest of a given dataset. Outliers can provide useful insights like potential errors and unique patterns. One particular anomaly detection method is the Isolation Forest method. Isolation Forest is a powerful anomaly detection method that operates by isolating outliers in a dataset. Unlike traditional methods, it utilizes a randomized and hierarchical approach, making it especially effective in identifying anomalies.\nIn the context of breast cancer detection, Isolation Forest can be applied to identify unusual or suspicious patterns within medical imaging data or specific characteristics related to tumors themselves. The code below performs a study that attempts to identify tumor radius and texture means outliers using the Isolation Forest method for a breast cancer dataset.\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Load the dataset\ndata = pd.read_csv('data.csv')\ndata.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n# Encode the 'diagnosis' column from categorical (Malignant or Benign) to numeric (1 or 0)\nlabel_encoder = LabelEncoder()\ndata['diagnosis'] = label_encoder.fit_transform(data['diagnosis'])\n\n\n# Define the feature columns (exclude the 'diagnosis' column)\nfeature_columns = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean']\n\n\n# Create a DataFrame with only the feature columns\nX = data[feature_columns]\n\n\n# Initialize the Isolation Forest model\nisolation_forest = IsolationForest(contamination=0.05)\n\n\n# Fit the model on the data\nisolation_forest.fit(X)\n\n/Users/rithvikguntor/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning:\n\nX does not have valid feature names, but IsolationForest was fitted with feature names\n\n\n\nIsolationForest(contamination=0.05)\n\n\n\n# Predict outliers using the trained model\noutliers = isolation_forest.predict(X)\noutliers\n\narray([-1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1, -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1,  1,  1,  1,  1,\n        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n        1,  1, -1,  1,  1,  1, -1, -1])\n\n\n\n# Create a new column in the original DataFrame to label the outliers\ndata['is_outlier'] = outliers\n\n\n# Filter the data to get the outliers, -1 indicates an outlier\noutlier_data = data[data['is_outlier'] == -1]\n\n\n# Visualize the data and outliers\nplt.figure(figsize=(12, 8))\n\n&lt;Figure size 1152x768 with 0 Axes&gt;\n\n\n&lt;Figure size 1152x768 with 0 Axes&gt;\n\n\n\n# Scatter plot of feature columns\nplt.scatter(data['radius_mean'], data['texture_mean'], label='Data Points', c='b', alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x7fdf2001cd30&gt;\n\n\n\n\n\nAfter fitting the Isolation Forest model and identifying outliers, we create a scatter plot of the feature columns, marking the data points in blue and the outliers in red with an ‘x’ marker. The scatter plot helps visualize the outliers in the context of the feature space, making it easier to understand their distribution.\n\n# Mark the outliers in red\nplt.scatter(outlier_data['radius_mean'], outlier_data['texture_mean'], label='Outliers', c='r', marker='x', s=100)\n\nplt.xlabel('Radius Mean')\nplt.ylabel('Texture Mean')\nplt.title('Isolation Forest Outlier Detection')\nplt.legend()\nplt.show()\n\n\n\n\nAfter plotting the data, we can take a closer look at the list of outliers.\n\n# Print the outliers\nprint(\"Outliers:\")\nprint(outlier_data)\n\nOutliers:\n            id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n0       842302          1       17.990         10.38          122.80   \n3     84348301          1       11.420         20.38           77.58   \n78     8610862          1       20.180         23.97          143.70   \n82     8611555          1       25.220         24.91          171.50   \n101     862722          0        6.981         13.43           43.79   \n108      86355          1       22.270         19.67          152.80   \n112      86409          0       14.260         19.65           97.83   \n122     865423          1       24.250         20.20          166.20   \n152    8710441          0        9.731         15.34           63.78   \n164    8712289          1       23.270         22.04          152.10   \n180     873592          1       27.220         21.87          182.10   \n181     873593          1       21.090         26.57          142.70   \n202     878796          1       23.290         26.67          158.90   \n212    8810703          1       28.110         18.47          188.50   \n236   88299702          1       23.210         26.97          153.50   \n257     886776          1       15.320         17.27          103.20   \n258     887181          1       15.660         23.20          110.20   \n339      89812          1       23.510         24.27          155.10   \n352     899987          1       25.730         17.46          174.20   \n400   90439701          1       17.910         21.02          124.40   \n461  911296202          1       27.420         26.27          186.90   \n503     915143          1       23.090         19.83          152.10   \n504     915186          0        9.268         12.87           61.49   \n521   91762702          1       24.630         21.60          165.50   \n538     921092          0        7.729         25.49           47.98   \n539     921362          0        7.691         25.44           48.34   \n563     926125          1       20.920         25.09          143.00   \n567     927241          1       20.600         29.33          140.10   \n568      92751          0        7.760         24.54           47.92   \n\n     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n0       1001.0          0.11840           0.27760         0.30010   \n3        386.1          0.14250           0.28390         0.24140   \n78      1245.0          0.12860           0.34540         0.37540   \n82      1878.0          0.10630           0.26650         0.33390   \n101      143.5          0.11700           0.07568         0.00000   \n108     1509.0          0.13260           0.27680         0.42640   \n112      629.9          0.07837           0.22330         0.30030   \n122     1761.0          0.14470           0.28670         0.42680   \n152      300.2          0.10720           0.15990         0.41080   \n164     1686.0          0.08439           0.11450         0.13240   \n180     2250.0          0.10940           0.19140         0.28710   \n181     1311.0          0.11410           0.28320         0.24870   \n202     1685.0          0.11410           0.20840         0.35230   \n212     2499.0          0.11420           0.15160         0.32010   \n236     1670.0          0.09509           0.16820         0.19500   \n257      713.3          0.13350           0.22840         0.24480   \n258      773.5          0.11090           0.31140         0.31760   \n339     1747.0          0.10690           0.12830         0.23080   \n352     2010.0          0.11490           0.23630         0.33680   \n400      994.0          0.12300           0.25760         0.31890   \n461     2501.0          0.10840           0.19880         0.36350   \n503     1682.0          0.09342           0.12750         0.16760   \n504      248.7          0.16340           0.22390         0.09730   \n521     1841.0          0.10300           0.21060         0.23100   \n538      178.8          0.08098           0.04878         0.00000   \n539      170.4          0.08668           0.11990         0.09252   \n563     1347.0          0.10990           0.22360         0.31740   \n567     1265.0          0.11780           0.27700         0.35140   \n568      181.0          0.05263           0.04362         0.00000   \n\n     concave_points_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n0                0.14710  ...          17.33           184.60      2019.0   \n3                0.10520  ...          26.50            98.87       567.7   \n78               0.16040  ...          31.72           170.30      1623.0   \n82               0.18450  ...          33.62           211.70      2562.0   \n101              0.00000  ...          19.54            50.41       185.2   \n108              0.18230  ...          28.01           206.80      2360.0   \n112              0.07798  ...          23.73           107.00       709.0   \n122              0.20120  ...          23.99           180.90      2073.0   \n152              0.07857  ...          19.49            71.04       380.5   \n164              0.09702  ...          28.22           184.20      2403.0   \n180              0.18780  ...          32.85           220.80      3216.0   \n181              0.14960  ...          33.48           176.50      2089.0   \n202              0.16200  ...          32.68           177.00      1986.0   \n212              0.15950  ...          18.47           188.50      2499.0   \n236              0.12370  ...          34.51           206.00      2944.0   \n257              0.12420  ...          22.66           119.80       928.8   \n258              0.13770  ...          31.64           143.70      1226.0   \n339              0.14100  ...          30.73           202.40      2906.0   \n352              0.19130  ...          23.58           229.30      3234.0   \n400              0.11980  ...          27.78           149.60      1304.0   \n461              0.16890  ...          31.37           251.20      4254.0   \n503              0.10030  ...          23.87           211.50      2782.0   \n504              0.05252  ...          16.38            69.05       300.2   \n521              0.14710  ...          26.93           205.70      2642.0   \n538              0.00000  ...          30.92            57.17       248.0   \n539              0.01364  ...          31.89            54.49       223.6   \n563              0.14740  ...          29.41           179.10      1819.0   \n567              0.15200  ...          39.42           184.60      1821.0   \n568              0.00000  ...          30.37            59.16       268.6   \n\n     smoothness_worst  compactness_worst  concavity_worst  \\\n0             0.16220            0.66560           0.7119   \n3             0.20980            0.86630           0.6869   \n78            0.16390            0.61640           0.7681   \n82            0.15730            0.60760           0.6476   \n101           0.15840            0.12020           0.0000   \n108           0.17010            0.69970           0.9608   \n112           0.08949            0.41930           0.6783   \n122           0.16960            0.42440           0.5803   \n152           0.12920            0.27720           0.8216   \n164           0.12280            0.35830           0.3948   \n180           0.14720            0.40340           0.5340   \n181           0.14910            0.75840           0.6780   \n202           0.15360            0.41670           0.7892   \n212           0.11420            0.15160           0.3201   \n236           0.14810            0.41260           0.5820   \n257           0.17650            0.45030           0.4429   \n258           0.15040            0.51720           0.6181   \n339           0.15150            0.26780           0.4819   \n352           0.15300            0.59370           0.6451   \n400           0.18730            0.59170           0.9034   \n461           0.13570            0.42560           0.6833   \n503           0.11990            0.36250           0.3794   \n504           0.19020            0.34410           0.2099   \n521           0.13420            0.41880           0.4658   \n538           0.12560            0.08340           0.0000   \n539           0.15960            0.30640           0.3393   \n563           0.14070            0.41860           0.6599   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     concave_points_worst  symmetry_worst  fractal_dimension_worst  is_outlier  \n0                  0.2654          0.4601                  0.11890          -1  \n3                  0.2575          0.6638                  0.17300          -1  \n78                 0.2508          0.5440                  0.09964          -1  \n82                 0.2867          0.2355                  0.10510          -1  \n101                0.0000          0.2932                  0.09382          -1  \n108                0.2910          0.4055                  0.09789          -1  \n112                0.1505          0.2398                  0.10820          -1  \n122                0.2248          0.3222                  0.08009          -1  \n152                0.1571          0.3108                  0.12590          -1  \n164                0.2346          0.3589                  0.09187          -1  \n180                0.2688          0.2856                  0.08082          -1  \n181                0.2903          0.4098                  0.12840          -1  \n202                0.2733          0.3198                  0.08762          -1  \n212                0.1595          0.1648                  0.05525          -1  \n236                0.2593          0.3103                  0.08677          -1  \n257                0.2229          0.3258                  0.11910          -1  \n258                0.2462          0.3277                  0.10190          -1  \n339                0.2089          0.2593                  0.07738          -1  \n352                0.2756          0.3690                  0.08815          -1  \n400                0.1964          0.3245                  0.11980          -1  \n461                0.2625          0.2641                  0.07427          -1  \n503                0.2264          0.2908                  0.07277          -1  \n504                0.1025          0.3038                  0.12520          -1  \n521                0.2475          0.3157                  0.09671          -1  \n538                0.0000          0.3058                  0.09938          -1  \n539                0.0500          0.2790                  0.10660          -1  \n563                0.2542          0.2929                  0.09873          -1  \n567                0.2650          0.4087                  0.12400          -1  \n568                0.0000          0.2871                  0.07039          -1  \n\n[29 rows x 33 columns]\n\n\nThe above code displays the outliers in the plot as well as the individual outlier data points outputted as a list. In the study, we looked at the radius_mean’, ‘texture_mean’, ‘perimeter_mean’, ‘area_mean’, ‘smoothness_mean’, ‘compactness_mean’, and ‘concavity_mean’ features. The Isolation Forest method used this subset of features in our data to identify outliers in the context of the texture_mean and radius_mean variables, as seen in the graph.\nBy efficiently isolating and identifying outliers within our dataset, this method has enabled us to extract valuable insights and uncover unusual patterns that may have otherwise remained hidden from us. The Isolation Forest’s capability to handle high-dimensional data, scalability, and ease of implementation make it a valuable asset in outlier and anomaly detection.\nGaussian Mixture Method\nAlong with the Isolation Forest method, there is another anomaly/outlier detection method known as the Gaussian Mixture model. We use the Gaussian Mixture Model (GMM) to fit the data and calculate Mahalanobis distances for each data point. Then, a threshold for anomaly detection is set based on the Mahalanobis distances. The threshold can be adjusted as needed. Data points are then labeled as an outlier or not depending on the comparison of their value to the threshold. The code below uses GMM to identify outliers based on Mahalanobis distances, which can be a powerful approach for anomaly detection. Adjust the number of components and threshold according to your specific dataset and requirements.\n\n# Performs outlier/anomaly detection using Gaussian Mixture Model\n# Initialize the Gaussian Mixture Model with a specified number of components (clusters) - We use 2 components to separate Malignant and Benign classes\nn_components = 2  \ngmm = GaussianMixture(n_components=n_components)\n\n\n# Fit the model on the data\ngmm.fit(X)\n\nGaussianMixture(n_components=2)\n\n\n\n# Calculate the Mahalanobis distances for each data point\nmahalanobis_distances = gmm.score_samples(X)\n\n\n# Set a threshold for anomaly detection (you can adjust this threshold)\nthreshold = np.percentile(mahalanobis_distances, 5)\n\n\n# Identify outliers based on the threshold\noutliers = mahalanobis_distances &lt; threshold\n\nWe identify outliers based on the threshold and create a new column ‘is_outlier’ in the original DataFrame to label the outliers.\n\n# Create a new column in the original DataFrame to label the outliers\ndata['is_outlier'] = outliers\n\n\n# Filter the data to get the outliers\noutlier_data = data[data['is_outlier']]\n\n\n# Visualize the data and outliers\nplt.figure(figsize=(12, 8))\n\n&lt;Figure size 1152x768 with 0 Axes&gt;\n\n\n&lt;Figure size 1152x768 with 0 Axes&gt;\n\n\n\n# Scatter plot of feature columns\nplt.scatter(data['radius_mean'], data['texture_mean'], label='Data Points', c='b', alpha=0.5)\n\n&lt;matplotlib.collections.PathCollection at 0x7fdf195b7c40&gt;\n\n\n\n\n\n\n# Mark the outliers in red\nplt.scatter(outlier_data['radius_mean'], outlier_data['texture_mean'], label='Outliers', c='r', marker='x', s=100)\n\nplt.xlabel('Radius Mean')\nplt.ylabel('Texture Mean')\nplt.title('Gaussian Mixture Model Outlier Detection')\nplt.legend()\nplt.show()\n\n\n\n\n\n# Print the outliers\nprint(\"Outliers:\")\nprint(outlier_data)\n\nOutliers:\n            id  diagnosis  radius_mean  texture_mean  perimeter_mean  \\\n0       842302          1       17.990         10.38          122.80   \n3     84348301          1       11.420         20.38           77.58   \n12      846226          1       19.170         24.80          132.40   \n68      859471          0        9.029         17.33           58.79   \n78     8610862          1       20.180         23.97          143.70   \n83     8611792          1       19.100         26.29          129.10   \n87    86135502          1       19.020         24.59          122.00   \n101     862722          0        6.981         13.43           43.79   \n108      86355          1       22.270         19.67          152.80   \n112      86409          0       14.260         19.65           97.83   \n122     865423          1       24.250         20.20          166.20   \n152    8710441          0        9.731         15.34           63.78   \n180     873592          1       27.220         21.87          182.10   \n181     873593          1       21.090         26.57          142.70   \n190     874858          1       14.220         23.12           94.37   \n202     878796          1       23.290         26.67          158.90   \n212    8810703          1       28.110         18.47          188.50   \n239   88330202          1       17.460         39.28          113.40   \n256   88649001          1       19.550         28.77          133.60   \n258     887181          1       15.660         23.20          110.20   \n318     894329          0        9.042         18.90           60.07   \n379    9013838          1       11.080         18.83           73.30   \n430     907914          1       14.900         22.53          102.10   \n461  911296202          1       27.420         26.27          186.90   \n504     915186          0        9.268         12.87           61.49   \n505     915276          0        9.676         13.14           64.12   \n538     921092          0        7.729         25.49           47.98   \n539     921362          0        7.691         25.44           48.34   \n568      92751          0        7.760         24.54           47.92   \n\n     area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\n0       1001.0          0.11840           0.27760         0.30010   \n3        386.1          0.14250           0.28390         0.24140   \n12      1123.0          0.09740           0.24580         0.20650   \n68       250.5          0.10660           0.14130         0.31300   \n78      1245.0          0.12860           0.34540         0.37540   \n83      1132.0          0.12150           0.17910         0.19370   \n87      1076.0          0.09029           0.12060         0.14680   \n101      143.5          0.11700           0.07568         0.00000   \n108     1509.0          0.13260           0.27680         0.42640   \n112      629.9          0.07837           0.22330         0.30030   \n122     1761.0          0.14470           0.28670         0.42680   \n152      300.2          0.10720           0.15990         0.41080   \n180     2250.0          0.10940           0.19140         0.28710   \n181     1311.0          0.11410           0.28320         0.24870   \n190      609.9          0.10750           0.24130         0.19810   \n202     1685.0          0.11410           0.20840         0.35230   \n212     2499.0          0.11420           0.15160         0.32010   \n239      920.6          0.09812           0.12980         0.14170   \n256     1207.0          0.09260           0.20630         0.17840   \n258      773.5          0.11090           0.31140         0.31760   \n318      244.5          0.09968           0.19720         0.19750   \n379      361.6          0.12160           0.21540         0.16890   \n430      685.0          0.09947           0.22250         0.27330   \n461     2501.0          0.10840           0.19880         0.36350   \n504      248.7          0.16340           0.22390         0.09730   \n505      272.5          0.12550           0.22040         0.11880   \n538      178.8          0.08098           0.04878         0.00000   \n539      170.4          0.08668           0.11990         0.09252   \n568      181.0          0.05263           0.04362         0.00000   \n\n     concave_points_mean  ...  texture_worst  perimeter_worst  area_worst  \\\n0                0.14710  ...          17.33           184.60      2019.0   \n3                0.10520  ...          26.50            98.87       567.7   \n12               0.11180  ...          29.94           151.70      1332.0   \n68               0.04375  ...          22.65            65.50       324.7   \n78               0.16040  ...          31.72           170.30      1623.0   \n83               0.14690  ...          32.72           141.30      1298.0   \n87               0.08271  ...          30.41           152.90      1623.0   \n101              0.00000  ...          19.54            50.41       185.2   \n108              0.18230  ...          28.01           206.80      2360.0   \n112              0.07798  ...          23.73           107.00       709.0   \n122              0.20120  ...          23.99           180.90      2073.0   \n152              0.07857  ...          19.49            71.04       380.5   \n180              0.18780  ...          32.85           220.80      3216.0   \n181              0.14960  ...          33.48           176.50      2089.0   \n190              0.06618  ...          37.18           106.40       762.4   \n202              0.16200  ...          32.68           177.00      1986.0   \n212              0.15950  ...          18.47           188.50      2499.0   \n239              0.08811  ...          44.87           141.20      1408.0   \n256              0.11440  ...          36.27           178.60      1926.0   \n258              0.13770  ...          31.64           143.70      1226.0   \n318              0.04908  ...          23.40            68.62       297.1   \n379              0.06367  ...          32.82            91.76       508.1   \n430              0.09711  ...          27.57           125.40       832.7   \n461              0.16890  ...          31.37           251.20      4254.0   \n504              0.05252  ...          16.38            69.05       300.2   \n505              0.07038  ...          18.04            69.47       328.1   \n538              0.00000  ...          30.92            57.17       248.0   \n539              0.01364  ...          31.89            54.49       223.6   \n568              0.00000  ...          30.37            59.16       268.6   \n\n     smoothness_worst  compactness_worst  concavity_worst  \\\n0             0.16220            0.66560           0.7119   \n3             0.20980            0.86630           0.6869   \n12            0.10370            0.39030           0.3639   \n68            0.14820            0.43650           1.2520   \n78            0.16390            0.61640           0.7681   \n83            0.13920            0.28170           0.2432   \n87            0.12490            0.32060           0.5755   \n101           0.15840            0.12020           0.0000   \n108           0.17010            0.69970           0.9608   \n112           0.08949            0.41930           0.6783   \n122           0.16960            0.42440           0.5803   \n152           0.12920            0.27720           0.8216   \n180           0.14720            0.40340           0.5340   \n181           0.14910            0.75840           0.6780   \n190           0.15330            0.93270           0.8488   \n202           0.15360            0.41670           0.7892   \n212           0.11420            0.15160           0.3201   \n239           0.13650            0.37350           0.3241   \n256           0.12810            0.53290           0.4251   \n258           0.15040            0.51720           0.6181   \n318           0.12210            0.37480           0.4609   \n379           0.21840            0.93790           0.8402   \n430           0.14190            0.70900           0.9019   \n461           0.13570            0.42560           0.6833   \n504           0.19020            0.34410           0.2099   \n505           0.20060            0.36630           0.2913   \n538           0.12560            0.08340           0.0000   \n539           0.15960            0.30640           0.3393   \n568           0.08996            0.06444           0.0000   \n\n     concave_points_worst  symmetry_worst  fractal_dimension_worst  is_outlier  \n0                  0.2654          0.4601                  0.11890        True  \n3                  0.2575          0.6638                  0.17300        True  \n12                 0.1767          0.3176                  0.10230        True  \n68                 0.1750          0.4228                  0.11750        True  \n78                 0.2508          0.5440                  0.09964        True  \n83                 0.1841          0.2311                  0.09203        True  \n87                 0.1956          0.3956                  0.09288        True  \n101                0.0000          0.2932                  0.09382        True  \n108                0.2910          0.4055                  0.09789        True  \n112                0.1505          0.2398                  0.10820        True  \n122                0.2248          0.3222                  0.08009        True  \n152                0.1571          0.3108                  0.12590        True  \n180                0.2688          0.2856                  0.08082        True  \n181                0.2903          0.4098                  0.12840        True  \n190                0.1772          0.5166                  0.14460        True  \n202                0.2733          0.3198                  0.08762        True  \n212                0.1595          0.1648                  0.05525        True  \n239                0.2066          0.2853                  0.08496        True  \n256                0.1941          0.2818                  0.10050        True  \n258                0.2462          0.3277                  0.10190        True  \n318                0.1145          0.3135                  0.10550        True  \n379                0.2524          0.4154                  0.14030        True  \n430                0.2475          0.2866                  0.11550        True  \n461                0.2625          0.2641                  0.07427        True  \n504                0.1025          0.3038                  0.12520        True  \n505                0.1075          0.2848                  0.13640        True  \n538                0.0000          0.3058                  0.09938        True  \n539                0.0500          0.2790                  0.10660        True  \n568                0.0000          0.2871                  0.07039        True  \n\n[29 rows x 33 columns]\n\n\nThe code above shows the outliers in the graph marked with an ‘X’. It can be seen that there are various similarities between the results from this graph as well as that from the Isolation Forest approach. There are various differences as well in the outlier results, and this is most likely a result of the differences in the outlier calculation approaches. Despite the differences in both approaches, the visualizations appear to display fairly accurate representations of the outlier data in both cases. Both approaches are powerful outlier detection techniques, and the choice between these methods is dependent on the structure of the data being worked with."
  }
]