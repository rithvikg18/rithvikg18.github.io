---
title: "Classification"
author: "Rithvik Guntor"
date: "2023-11-1"
categories: 
  - code
  - analysis
jupyter:
  jupytext:
    formats: 'ipynb,qmd'
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec: 
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---
**Random Forest Classification**

Classification is state-of-the-art supervised machine learning algorithm that is often used to categorize/classify data into distinct classes based on their attributes. These algorithms require labeled data for training purposes. This subset of machine learning is often used in healthcare, and we will take a dive into how this concept can be applied in breast cancer research/modeling. There are various classification algorithms that can be used depending on the problem at hand, however the one we will take a closer look at for now is Random Forest classification. Random Forest classification makes predictions about categories or classes for data points using a collection of decision trees. These trees collaborate to develop increasingly reliable results using the data features. The code below explores how Random Forest uses breast cancer data to classify a patient tumor as benign or malignant. 
```{python}
# We import our necessary libraries/modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
```
We are using Python's built-in breast cancer dataset for this study.
```{python}
# Load in sci-kit learn's breast cancer dataset
data = load_breast_cancer()
data
```

```{python}
# Split up our X-value data
X = pd.DataFrame(data.data, columns = data.feature_names)
X
```

```{python}
# Split up our y-value data
y = pd.Series(data.target)
y
```

```{python}
# Split our data into training and testing data to develop our model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

```{python}
# Initialize the model
clf = RandomForestClassifier(n_estimators=100, random_state=42)
```

```{python}
# Fit our model usin the training data
clf.fit(X_train, y_train)
```

```{python}
# Make predictions using the X-testing data
y_pred = clf.predict(X_test)
y_pred
```
After making predictions with the model, we want to verify its accuracy by comparing the predicted results with the actual test results. 
```{python}
# Compute the accuracy based on our calculated y-data and the y-test data
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
```
We can visualize the effect of each feature to better understand the most significant features in the Random Forest study.
```{python}
# Sort feature importances for visualization purposes
feature_importances = clf.feature_importances_
sorted_indices = np.argsort(feature_importances)[::-1]
sorted_features = X.columns[sorted_indices]
```

```{python}
# Plot the sorted feature importances in descending order to view the most significant features
plt.figure(figsize=(12, 6))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), feature_importances[sorted_indices], align="center")
plt.xticks(range(X.shape[1]), sorted_features, rotation=90)
plt.tight_layout()
plt.show()
```
We can see that from the code output above, the model appears to be fairly accurate with an accuracy score of 0.97. The model appears to correctly classify the data for about 97 percent of the samples, and misclassifying 3 percent. These results indicate that the Random Forest model performs well on the dataset by correctly predicting the majority of the sample data. Let's take a closer look at which features in the model likely contributed to these results. In the above visual, we graphed the importance of each of the features to see which ones were the most significant in the model. It can be seen that "mean concave points" and "worst concave points" appear to be the most influential features in the model. 

**Support Vector Machine Classification**

As it was seen that the results that were yielded from the Random Forest model were accurate, we can experiment with another classification model to see if we can achieve even more accurate results. The Support Vector Machine (SVM) algorithm is a classification algorithm in which a line or boundary is draw to separate different types of data points in a way that maximizes the gap. The code below performs the same analysis using the SVM algorithm with the goal of more accurate results. 

```{python}
# Import the SVM fro sci-kit learn and initialize a model
from sklearn.svm import SVC

svm_classifier = SVC(kernel='linear', random_state=42)
```

```{python}
# Fit the model using the training data
svm_classifier.fit(X_train, y_train)
```

```{python}
# Make predictions with the newly fitted model using the X-test data
y_pred = svm_classifier.predict(X_test)
y_pred
```
After making predictions with the model, we want to verify its accuracy by comparing the predicted results with the actual test results.
```{python}
# Compute the accuracy based on our calculated y-data and the y-test data
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
```

```{python}
# Compute/generate the feature importances
feature_importances = np.abs(svm_classifier.coef_)[0]
feature_importances
```

```{python}
# Sort the feature importances in descending order
df = pd.DataFrame({'names': data.feature_names, 'importance':feature_importances})
df_sorted = df.sort_values(by='importance', ascending=False)
df_sorted
```
We can visualize the effect of each feature to better understand the most significant features in the SVM study.
```{python}
# Plot the sorted feature importances in descending order to view the most significant features
plt.figure(figsize=(10, 6))
plt.bar(df_sorted.names, df_sorted.importance, color='skyblue')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importances (SVM)')
plt.xticks(range(X.shape[1]), sorted_features, rotation=90)
plt.show()
```
It can be seen that from the results above, we have an accuracy score of about 0.96. The Random Forest model appears to be a better fit for this data as it yielded a slightly higher accuracy score of 0.97. However, even though that the Random Forest model yielded higher accuracy score, the SVM model appears to still be fairly accurate as it correctly predicts 96 percent of the sample data. Additionally, it can be seen in the above visualization that the "mean concave points" and "worst concave points" are the most influential features in our SVM model, which was the same case with our Random Forest model. These features appear to be the most significant features in tumor classification in this study. Ultimately, both the Random Forest and SVM models are fairly accurate with tumor prediction, however the Random Forest model appeared to be a slightly more accurate predictor in this study. 